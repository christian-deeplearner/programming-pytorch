{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./train\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Compose(\n    Resize(size=(64, 64), interpolation=PIL.Image.BILINEAR)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_path = \"./val/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)\n",
    "\n",
    "test_data_path = \"./test/\"\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data_loader = data.DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader = data.DataLoader(val_data, batch_size=batch_size)\n",
    "test_data_loader = data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([64, 3, 64, 64]), torch.Size([64]))"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "image.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(12288, 84)\n",
    "        self.fc2 = nn.Linear(84, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 12288)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SimpleNet(\n  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n  (fc2): Linear(in_features=84, out_features=50, bias=True)\n  (fc3): Linear(in_features=50, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "simplenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(simplenet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    print(device)\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input, target = batch\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.data.item()\n",
    "        training_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "\n",
    "        for batch in val_loader:\n",
    "            input, target = batch\n",
    "            input = input.to(device)\n",
    "            output = model(input)\n",
    "\n",
    "            target = target.to(device)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            valid_loss += loss.data.item()\n",
    "\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],\n",
    "\t\t\t\t\t\t\t   target).view(-1)\n",
    "\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "\n",
    "        valid_loss /= len(val_loader)\n",
    "\n",
    "        print(\"Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}\".format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\nEpoch: 0, Training Loss: 3.22, Validation Loss: 2.61, accuracy = 0.41\nEpoch: 1, Training Loss: 1.97, Validation Loss: 1.58, accuracy = 0.67\nEpoch: 2, Training Loss: 1.24, Validation Loss: 0.84, accuracy = 0.61\nEpoch: 3, Training Loss: 0.60, Validation Loss: 0.65, accuracy = 0.73\nEpoch: 4, Training Loss: 0.52, Validation Loss: 0.62, accuracy = 0.68\nEpoch: 5, Training Loss: 0.39, Validation Loss: 0.66, accuracy = 0.70\nEpoch: 6, Training Loss: 0.36, Validation Loss: 0.63, accuracy = 0.72\nEpoch: 7, Training Loss: 0.32, Validation Loss: 0.63, accuracy = 0.72\nEpoch: 8, Training Loss: 0.29, Validation Loss: 0.63, accuracy = 0.72\nEpoch: 9, Training Loss: 0.26, Validation Loss: 0.66, accuracy = 0.72\nEpoch: 10, Training Loss: 0.24, Validation Loss: 0.65, accuracy = 0.71\nEpoch: 11, Training Loss: 0.22, Validation Loss: 0.67, accuracy = 0.71\nEpoch: 12, Training Loss: 0.20, Validation Loss: 0.68, accuracy = 0.71\nEpoch: 13, Training Loss: 0.18, Validation Loss: 0.70, accuracy = 0.72\nEpoch: 14, Training Loss: 0.16, Validation Loss: 0.73, accuracy = 0.72\nEpoch: 15, Training Loss: 0.14, Validation Loss: 0.75, accuracy = 0.71\nEpoch: 16, Training Loss: 0.12, Validation Loss: 0.84, accuracy = 0.68\nEpoch: 17, Training Loss: 0.13, Validation Loss: 0.77, accuracy = 0.72\nEpoch: 18, Training Loss: 0.10, Validation Loss: 0.82, accuracy = 0.70\nEpoch: 19, Training Loss: 0.09, Validation Loss: 0.82, accuracy = 0.72\n"
    }
   ],
   "source": [
    "train(simplenet, optimizer, torch.nn.CrossEntropyLoss(),train_data_loader, val_data_loader, 20, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(simplenet, \"simplenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(3, 64, 64)"
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "img.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 64), <f4",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2680\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 64), '<f4')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-4b9b928629f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2680\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 64), <f4"
     ]
    }
   ],
   "source": [
    "img = Image.fromarray(img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_data_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['cat', 'fish']"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = \"/Users/christian_acuna/workspace/beginners-pytorch-deep-learning/chapter2/test/cat/3156111_a9dba42579.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish = \"/Users/christian_acuna/workspace/beginners-pytorch-deep-learning/chapter2/test/fish/35099721_fbc694fa1b.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def inference(filename, classes, model):\n",
    "    labels = ['cat', 'fish']\n",
    "    img = Image.open(filename)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    prediction = model(img)\n",
    "    prediction = prediction.argmax()\n",
    "    print(classes[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cat\n"
    }
   ],
   "source": [
    "inference(cat, classes, simplenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "fish\n"
    }
   ],
   "source": [
    "inference(fish, classes, simplenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(\"simplenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SimpleNet(\n  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n  (fc2): Linear(in_features=84, out_features=50, bias=True)\n  (fc3): Linear(in_features=50, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}